{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn.externals import joblib\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# XGBoost params:\n",
    "xgboost_params = { \n",
    "   \"objective\": \"binary:logistic\",\n",
    "   \"booster\": \"gbtree\",\n",
    "   \"eval_metric\": \"auc\",\n",
    "   \"eta\": 0.01, # 0.06, #0.01,\n",
    "   #\"min_child_weight\": 240,\n",
    "   \"subsample\": 0.75,\n",
    "   \"colsample_bytree\": 0.68,\n",
    "   \"max_depth\": 7\n",
    "}\n",
    "\n",
    "print('Load data...')\n",
    "train = pd.read_csv('data/train.csv')\n",
    "target = train['target']\n",
    "train = train.drop(['ID','target'],axis=1)\n",
    "test = pd.read_csv('data/test.csv')\n",
    "ids = test['ID'].values\n",
    "test = test.drop(['ID'],axis=1)\n",
    "#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function, dummify, that will replace categorical features with dummy columns. Return the new dataset,\n",
    "# the names of the dummy columns, and the rows with null values for each categorical variable\n",
    "def dummify(name,series):\n",
    "    prefix_string = name + '_'\n",
    "    dummies = pd.get_dummies(series,prefix=prefix_string)\n",
    "    dummy_column_names = dummies.columns.values\n",
    "    #Get a list of all rows containing nulls. After dummifying these rows will just have all zeros for dummy variable\n",
    "    get_nulls = np.where(series.isnull() == True)[0].tolist()\n",
    "\n",
    "    return dummies, dummy_column_names, get_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing...\n",
      "Dummifying v3\n",
      "Dummifying v24\n",
      "Dummifying v30\n",
      "Dummifying v31\n",
      "Dummifying v47\n",
      "Dummifying v52\n",
      "Dummifying v56\n",
      "Dummifying v66\n",
      "Dummifying v71\n",
      "Dummifying v74\n",
      "Dummifying v75\n",
      "Dummifying v79\n",
      "Dummifying v91\n",
      "Dummifying v107\n",
      "Dummifying v110\n",
      "Dummifying v112\n",
      "Dummifying v113\n",
      "Dummifying v125\n"
     ]
    }
   ],
   "source": [
    "print('Clearing...')\n",
    "\n",
    "train_dummy_columns = {}\n",
    "train_nulls_dict = {}\n",
    "test_dummy_columns = {}\n",
    "test_nulls_dict = {}\n",
    "\n",
    "variables_to_keep = ['v3', 'v10', 'v12', 'v14', 'v21', 'v22', 'v24','v31', 'v34', 'v38', 'v40', 'v47', 'v50', 'v52',\n",
    "                     'v56', 'v62','v66', 'v71', 'v72', 'v74', 'v75', 'v79', 'v91', 'v107','v110', 'v112', \n",
    "                     'v114', 'v125', â€™v129']\n",
    "cleaned_train = train[variables_to_keep].copy()\n",
    "cleaned_test = test.copy()\n",
    "\n",
    "\n",
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_name == 'v22':\n",
    "        #v22 has too many options to dummify, instead: factorize\n",
    "        cleaned_train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        cleaned_test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)    \n",
    "    elif train_series.dtype == 'O':\n",
    "        print 'Dummifying ' + train_name\n",
    "        cleaned_train.drop(train_name,axis=1,inplace = True)\n",
    "        cleaned_test.drop(train_name,axis=1,inplace = True)\n",
    "        \n",
    "        train_dummies, train_dummy_list, train_null_list = dummify(train_name,train_series)\n",
    "        test_dummies, test_dummy_list, test_null_list = dummify(test_name,test_series)\n",
    "\n",
    "        cleaned_train = pd.concat([cleaned_train,train_dummies], axis = 1)\n",
    "        cleaned_test = pd.concat([cleaned_test,test_dummies], axis = 1)\n",
    "\n",
    "        train_dummy_columns[train_name] = train_dummy_list\n",
    "        train_nulls_dict[train_name] = train_null_list\n",
    "        test_dummy_columns[test_name] = test_dummy_list\n",
    "        test_nulls_dict[test_name] = test_null_list\n",
    "        \n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            cleaned_train.loc[train_series.isnull(), train_name] = train_series.mean()\n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            cleaned_test.loc[test_series.isnull(), test_name] = train_series.mean()  #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit the model...\n",
      "Training....\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(cleaned_train.values, target.values, test_size=0.15, random_state = 1)\n",
    "\n",
    "final = False\n",
    "\n",
    "# xgtrain = xgb.DMatrix(train.values, target.values)\n",
    "# xgtest = xgb.DMatrix(test.values)\n",
    "\n",
    "xgtrain = xgb.DMatrix(X_train, y_train)\n",
    "xgvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "xgtest = xgb.DMatrix(cleaned_test.values)\n",
    "\n",
    "if final == True:\n",
    "    xgtrain = xgb.DMatrix(cleaned_train.values, target.values)\n",
    "    \n",
    "\n",
    "#Now let's fit the model\n",
    "print('Fit the model...')\n",
    "boost_round = 2500 #1800 CHANGE THIS BEFORE START\n",
    "\n",
    "filename = 'data/model/xgb-dummy-except-v22_submit.pkl'\n",
    "if not os.path.exists(filename):\n",
    "    print 'Training....'\n",
    "    eval_dict = dict()\n",
    "    if final == False:\n",
    "        clf = xgb.train(xgboost_params, xgtrain, num_boost_round=boost_round, verbose_eval=50, maximize=False,\n",
    "                    evals_result=eval_dict, early_stopping_rounds=100, evals=[(xgvalid, 'valid')])\n",
    "    else:\n",
    "    # Use the following when not using validation a.k.a. for final run\n",
    "        clf = xgb.train(xgboost_params,xgtrain,num_boost_round=boost_round,verbose_eval=True,maximize=False)\n",
    "    joblib.dump(clf, filename)\n",
    "else:\n",
    "    print 'Loading...'\n",
    "    clf = joblib.load(filename)\n",
    "\n",
    "# ------------------\n",
    "#print(eval_dict)\n",
    "if final == False:\n",
    "    score_list = [float(s) for s in eval_dict['valid']['auc']]\n",
    "    print(max(score_list))   # There are best score (max() - for 'auc', but min() for logloss)\n",
    "    #same result:\n",
    "    print(clf.best_score) \n",
    "    #and best xgboost_round:\n",
    "    print(clf.best_iteration)\n",
    "\n",
    "\n",
    "    plt.plot(score_list)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#----------------    \n",
    "# Code for predicting on test data and saving prediction result\n",
    "# Make predict\n",
    "print('Predict...')\n",
    "filename = 'data/model/xgb-dummy-except-v22_submit.pkl'\n",
    "clf = joblib.load(filename)\n",
    "test_preds = clf.predict(xgtest, ntree_limit=clf.best_iteration)\n",
    "# Save results\n",
    "#\n",
    "predictions_file = open(\"data/team_GAF_result.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow([\"ID\", \"PredictedProb\"])\n",
    "open_file_object.writerows(zip(ids, test_preds))\n",
    "predictions_file.close()\n",
    "#\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
