{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random; random.seed(2016)\n",
    "import time; start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Load data...')\n",
    "train = pd.read_csv('data/train.csv')\n",
    "target = train['target']\n",
    "train = train.drop(['ID','target'],axis=1)\n",
    "test = pd.read_csv('data/test.csv')\n",
    "id_test = test['ID']\n",
    "test = test.drop(['ID'],axis=1)\n",
    "num_train = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function, dummify, that will replace categorical features with dummy columns. Return the new dataset,\n",
    "# the names of the dummy columns, and the rows with null values for each categorical variable\n",
    "def dummify(name,series):\n",
    "    prefix_string = name + '_'\n",
    "    dummies = pd.get_dummies(series,prefix=prefix_string)\n",
    "    dummy_column_names = dummies.columns.values\n",
    "    #Get a list of all rows containing nulls. After dummifying these rows will just have all zeros for dummy variable\n",
    "    get_nulls = np.where(series.isnull() == True)[0].tolist()\n",
    "\n",
    "    return dummies, dummy_column_names, get_nulls\n",
    "\n",
    "def fill_nan_null(val):\n",
    "    ret_fill_nan_null = 0.0\n",
    "    if val == True:\n",
    "        ret_fill_nan_null = 1.0\n",
    "    return ret_fill_nan_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop features with > 0.9 correlation. Keep feature with fewest NaNs\n",
    "\n",
    "corr = train.corr()\n",
    "\n",
    "to_drop = set()\n",
    "\n",
    "for col in corr.columns.values:\n",
    "    if col in to_drop:\n",
    "        continue\n",
    "\n",
    "    col_list = corr[col][(corr[col] > 0.9) & (corr[col] < 1)].index.tolist()\n",
    "    col_set = set(col_list)\n",
    "    col_set.difference_update(to_drop)\n",
    "    if (len(col_list) == 0) or (len(col_set) == 0):\n",
    "        continue\n",
    "\n",
    "    col_list.append(col)\n",
    "    lowest_na_count = train[col_list[0]].isnull().sum()\n",
    "    best_col = col_list[0]\n",
    "    for option in col_list:\n",
    "        na_count = train[option].isnull().sum()\n",
    "        if na_count < lowest_na_count:\n",
    "            lowest_na_count = na_count\n",
    "            best_col = option\n",
    "    col_list.remove(best_col)\n",
    "    to_drop.update(col_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114393, 131)\n",
      "(114321, 131)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding features...\n"
     ]
    }
   ],
   "source": [
    "print \"Adding features...\"\n",
    "\n",
    "train_test_data = pd.concat([train,test],axis=0, ignore_index=True)\n",
    "\n",
    "train_test_data_types = train_test_data.dtypes[:]\n",
    "\n",
    "\n",
    "for i in range(len(train_test_data_types)):\n",
    "    train_test_data[str(train_test_data_types.index[i])+'_nan_'] = train_test_data[str(train_test_data_types.index[i])].map(lambda x:fill_nan_null(pd.isnull(x)))\n",
    "    \n",
    "train_test_data['NA_num'] = train_test_data.isnull().sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing...\n",
      "Dummifying v3\n",
      "Dummifying v24\n",
      "Dummifying v30\n",
      "Dummifying v31\n",
      "Dummifying v47\n",
      "Dummifying v52\n",
      "Dummifying v56\n",
      "Dummifying v66\n",
      "Dummifying v71\n",
      "Dummifying v74\n",
      "Dummifying v75\n",
      "Dummifying v79\n",
      "Dummifying v91\n",
      "Dummifying v107\n",
      "Dummifying v110\n",
      "Dummifying v112\n",
      "Dummifying v113\n",
      "Dummifying v125\n"
     ]
    }
   ],
   "source": [
    "drop_correlated = True\n",
    "\n",
    "if drop_correlated == True:\n",
    "    print 'Drop Correlated ...'\n",
    "    train_test_data.drop(list(to_drop),axis=1, inplace = True)\n",
    "\n",
    "\n",
    "print('Clearing...')\n",
    "train_dummy_columns = {}\n",
    "train_nulls_dict = {}\n",
    "test_dummy_columns = {}\n",
    "test_nulls_dict = {}\n",
    "\n",
    "cleaned_train_test_data = train_test_data.copy()\n",
    "\n",
    "\n",
    "for (train_name, train_series) in train_test_data.iteritems():\n",
    "    if train_name == 'v22':\n",
    "        #v22 has too many options to dummify, instead: factorize\n",
    "        cleaned_train_test_data[train_name], tmp_indexer = pd.factorize(train_test_data[train_name])\n",
    "        #but now we have -1 values (NaN)    \n",
    "    elif train_series.dtype == 'O':\n",
    "        print 'Dummifying ' + train_name\n",
    "        cleaned_train_test_data.drop(train_name,axis=1,inplace = True)\n",
    "\n",
    "        train_dummies, train_dummy_list, train_null_list = dummify(train_name,train_series)\n",
    "        cleaned_train_test_data = pd.concat([cleaned_train_test_data,train_dummies], axis = 1)\n",
    "        \n",
    "        train_dummy_columns[train_name] = train_dummy_list\n",
    "        train_nulls_dict[train_name] = train_null_list\n",
    "        \n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train_test_data[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            cleaned_train_test_data.loc[train_series.isnull(), train_name] = train_series.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cleaned_train.isnull().sum(axis=1)\n",
    "# with pd.option_context('display.max_rows', 999, 'display.max_columns', 3):\n",
    "#    print cleaned_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228714, 228)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = cleaned_train_test_data.iloc[:num_train]\n",
    "test = cleaned_train_test_data.iloc[num_train:]\n",
    "\n",
    "\n",
    "y_train = target\n",
    "\n",
    "\n",
    "def flog_loss(ground_truth, predictions):\n",
    "    flog_loss_ = log_loss(ground_truth, predictions) #, eps=1e-15, normalize=True, sample_weight=None)\n",
    "    return flog_loss_\n",
    "LL  = make_scorer(flog_loss, greater_is_better=False)\n",
    "\n",
    "g={'ne':500,'md':40,'mf':60,'rs':2016}\n",
    "etc = ensemble.ExtraTreesClassifier(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], random_state=g['rs'], criterion='entropy', min_samples_split= 4, min_samples_leaf= 2, verbose = 0, n_jobs =-1)      \n",
    "etr = ensemble.ExtraTreesRegressor(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], random_state=g['rs'], min_samples_split= 4, min_samples_leaf= 2, verbose = 0, n_jobs =-1)      \n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], random_state=g['rs'], criterion='entropy', min_samples_split= 4, min_samples_leaf= 2, verbose = 0, n_jobs =-1)\n",
    "rfr = ensemble.RandomForestRegressor(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], random_state=g['rs'], min_samples_split= 4, min_samples_leaf= 2, verbose = 0, n_jobs =-1)\n",
    "xgr = xgb.XGBRegressor(n_estimators=g['ne'], max_depth=g['md'], seed=g['rs'], missing=np.nan, learning_rate=0.02, subsample=0.9, colsample_bytree=0.85, objective='reg:linear')\n",
    "xgc = xgb.XGBClassifier(n_estimators=g['ne'], max_depth=g['md'], seed=g['rs'], missing=np.nan, learning_rate=0.02, subsample=0.9, colsample_bytree=0.85, objective='binary:logistic') \n",
    "clf = {'etc':etc, 'etr':etr, 'rfc':rfc, 'rfr':rfr, 'xgr':xgr, 'xgc':xgc} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114393, 586)\n",
      "(114321, 586)\n"
     ]
    }
   ],
   "source": [
    "print test.shape\n",
    "print train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ensemble Model: ', 'etr', ' Best CV score: ', -0.47234427554051223, ' Time: ', 25.42)\n",
      "('Ensemble Model: ', 'rfr', ' Best CV score: ', -0.46986564580673634, ' Time: ', 43.75)\n",
      "('Ensemble Model: ', 'xgc', ' Best CV score: ', -0.25415441861034482, ' Time: ', 81.91)\n",
      "('Ensemble Model: ', 'rfc', ' Best CV score: ', -0.19538655725974971, ' Time: ', 106.91)\n",
      "('Ensemble Model: ', 'etc', ' Best CV score: ', -0.16260258019671914, ' Time: ', 154.65)\n",
      "('Ensemble Model: ', 'xgr', ' Best CV score: ', nan, ' Time: ', 209.57)\n"
     ]
    }
   ],
   "source": [
    "y_pred=[]\n",
    "best_score = 0.0\n",
    "id_results = id_test[:]\n",
    "for c in clf:\n",
    "    if c[:1] != \"x\": #not xgb\n",
    "        model = GridSearchCV(estimator=clf[c], param_grid={}, n_jobs =-1, cv=2, verbose=0, scoring=LL)\n",
    "        model.fit(train, y_train.values)\n",
    "        if c[-1:] != \"c\": #not classifier\n",
    "            y_pred = model.predict(test)\n",
    "            print(\"Ensemble Model: \", c, \" Best CV score: \", model.best_score_, \" Time: \", round(((time.time() - start_time)/60),2))\n",
    "        else: #classifier\n",
    "            best_score = (log_loss(y_train.values, model.predict_proba(train)))*-1\n",
    "            y_pred = model.predict_proba(test)[:,1]\n",
    "            print(\"Ensemble Model: \", c, \" Best CV score: \", best_score, \" Time: \", round(((time.time() - start_time)/60),2))\n",
    "    else: #xgb\n",
    "        X_fit, X_eval, y_fit, y_eval= train_test_split(train, y_train, test_size=0.35, train_size=0.65, random_state=g['rs'])\n",
    "        model = clf[c]\n",
    "        model.fit(X_fit, y_fit.values, early_stopping_rounds=20, eval_metric=\"logloss\", eval_set=[(X_eval, y_eval)], verbose=0)\n",
    "        if c == \"xgr\": #xgb regressor\n",
    "            best_score = (log_loss(y_train.values, model.predict(train)))*-1\n",
    "            y_pred = model.predict(test)\n",
    "        else: #xgb classifier\n",
    "            best_score = (log_loss(y_train.values, model.predict_proba(train)))*-1\n",
    "            y_pred = model.predict_proba(test)[:,1]\n",
    "        print(\"Ensemble Model: \", c, \" Best CV score: \", best_score, \" Time: \", round(((time.time() - start_time)/60),2))\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i]<0.0:\n",
    "            y_pred[i] = 0.0\n",
    "        if y_pred[i]>1.0:\n",
    "            y_pred[i] = 1.0\n",
    "    df_in = pd.DataFrame({\"ID\": id_test, c: y_pred})\n",
    "    id_results = pd.concat([id_results, df_in[c]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 114393)\n",
      "(1, 114393)\n",
      "(2, 114393)\n",
      "(3, 114393)\n",
      "(4, 114393)\n",
      "(5, 114393)\n",
      "(6, 114393)\n",
      "(7, 114393)\n",
      "(8, 114393)\n",
      "(9, 114393)\n"
     ]
    }
   ],
   "source": [
    "id_results['avg'] = id_results.drop('ID', axis=1).apply(np.average, axis=1)\n",
    "id_results['min'] = id_results.drop('ID', axis=1).apply(min, axis=1)\n",
    "id_results['max'] = id_results.drop('ID', axis=1).apply(max, axis=1)\n",
    "id_results['diff'] = id_results['max'] - id_results['min']\n",
    "for i in range(10):\n",
    "    print(i, len(id_results[id_results['diff']>(i/10)]))\n",
    "id_results.to_csv(\"results_analysis.csv\", index=False)\n",
    "ds = id_results[['ID','avg']]\n",
    "ds.columns = ['ID','PredictedProb']\n",
    "ds.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PredictedProb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.472591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.808729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.798940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.530312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.796838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  PredictedProb\n",
       "0   0       0.472591\n",
       "1   1       0.808729\n",
       "2   2       0.798940\n",
       "3   7       0.530312\n",
       "4  10       0.796838"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
