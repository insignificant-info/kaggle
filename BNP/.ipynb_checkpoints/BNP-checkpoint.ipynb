{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn import decomposition, linear_model, grid_search, metrics, ensemble\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in data\n",
    "train_df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a training dataset. Features (x) should include all columns except target and ID. Target (y) should only \n",
    "#include the target column\n",
    "train_x = train_df.drop(['target','ID'], axis=1)\n",
    "train_x.head()\n",
    "train_y = train_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find which feaetures have categorical values (dtype = object) and which have continuous values (dtype = float)\n",
    "# Save the names of categorical feature columns to categorical_columns\n",
    "g = train_x.columns.to_series().groupby(train_x.dtypes).groups \n",
    "g_keys = g.keys()\n",
    "categorical_columns = g[g_keys[0]]\n",
    "# categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the mean of the target variable to look for class imbalance. The mean is ~ 0.76 so ~76% of data is \n",
    "# target value 1\n",
    "train_df.target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the number of values that each categorical feature can have\n",
    "for variable in categorical_columns:\n",
    "    print variable + \" \" +  str(len(train_df[variable].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable 22 does not seem useable. It has 18,211 unique categorical variables. Going to drop it unless I find a better way to work with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove variable 22\n",
    "train_x.drop('v22',axis = 1, inplace = True)\n",
    "g = train_x.columns.to_series().groupby(train_x.dtypes).groups \n",
    "g_keys = g.keys()\n",
    "categorical_columns = g[g_keys[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function, dummify, that will replace categorical features with dummy columns. Return the new dataset,\n",
    "# the names of the dummy columns, and the rows with null values for each categorical variable\n",
    "def dummify(column_name,dataset):\n",
    "    prefix_string = column_name + '_'\n",
    "    dummies = pd.get_dummies(train_x[column_name],prefix=prefix_string)\n",
    "    dummy_column_names = dummies.columns.values\n",
    "    #Get a list of all rows containing nulls. After dummifying these rows will just have all zeros for dummy variable\n",
    "    get_nulls = np.where(dataset[column_name].isnull() == True)[0].tolist()\n",
    "\n",
    "    dataset.drop(column_name, axis = 1, inplace = True)\n",
    "\n",
    "    return pd.concat([dataset,dummies], axis = 1), dummy_column_names, get_nulls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Iterate through each categorical variable. On each iteration, call dummify for that variable and load\n",
    "# column names and null list into a dictionary\n",
    "dummified_train_x = train_x.copy()\n",
    "dummy_columns = {}\n",
    "nulls_dict = {}\n",
    "for column in categorical_columns:\n",
    "    print 'Dummifying ' + column\n",
    "    dummified_train_x,temp_dummy_list, temp_null_list = dummify(column,dummified_train_x)\n",
    "    dummy_columns[column] = temp_dummy_list\n",
    "    nulls_dict[column] = temp_null_list\n",
    "    \n",
    "\n",
    "# Check which categorical variables have missing (null) values that will need to be filled, track those features in\n",
    "# variables_to_predict\n",
    "variables_to_predict = []\n",
    "for key in nulls_dict:\n",
    "    print key + ' ' + str(len(nulls_dict[key]))\n",
    "    if len(nulls_dict[key]) > 0:\n",
    "        variables_to_predict.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print a sample row to see dummified features are correctly in place\n",
    "#with pd.option_context('display.max_rows', 999, 'display.max_columns', 3):\n",
    "#    print dummified_train_x.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp = Imputer(strategy='mean', axis=0)\n",
    "temp = imp.fit_transform(dummified_train_x)\n",
    "\n",
    "imputed_dummified_train_x = pd.DataFrame(temp)\n",
    "imputed_dummified_train_x.columns = dummified_train_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function to predict missing categorical values\n",
    "# Impute the mean for missing continuous variables\n",
    "# May change in the future to also predict continuous variables\n",
    "def predict_values(column_name,dataset,nulls_list, variable, force=False):\n",
    "\n",
    "    dataset_test = dataset.iloc[nulls_list]\n",
    "    dataset_test_target = dataset_test[column_name]\n",
    "    dataset_test_features = dataset_test.drop(column_name,axis=1)\n",
    "\n",
    "    dataset_train = dataset.drop(nulls_list)\n",
    "    dataset_train_target = dataset_train[column_name]\n",
    "    dataset_train_features = dataset_train.drop(column_name,axis=1)\n",
    "\n",
    "    filename = 'data/model/model_' + variable+'.pkl'\n",
    "    if force or not os.path.exists(filename):\n",
    "        print 'Training ' + variable\n",
    "        estimator = Pipeline([(\"imputer\", Imputer(strategy=\"mean\",axis=0)),\n",
    "                      (\"forest\", RandomForestRegressor(random_state=0,n_estimators=10))])\n",
    "        estimator.fit(dataset_train_features, dataset_train_target)\n",
    "        score = cross_val_score(estimator, dataset_train_features, dataset_train_target).mean()\n",
    "        print(\"Score with the entire dataset = %.2f\" % score)\n",
    "        joblib.dump(estimator, filename)\n",
    "    else:\n",
    "        print 'Loading ' + variable\n",
    "        estimator = joblib.load(filename)\n",
    "\n",
    "    dataset.loc[nulls_list,column_name] = estimator.predict(dataset_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict all categorical missing values\n",
    "for variable in variables_to_predict:\n",
    "    predict_values(dummy_columns[variable].tolist(),imputed_dummified_train_x, nulls_dict[variable], variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try initial params and approach from Dmitry M. \n",
    "# https://www.kaggle.com/director/bnp-paribas-cardif-claims-management/simple-xgboost-0-46146\n",
    "\n",
    "\n",
    "\n",
    "# XGBoost params:\n",
    "xgboost_params = { \n",
    "   \"objective\": \"binary:logistic\",\n",
    "   \"booster\": \"gbtree\",\n",
    "   \"eval_metric\": \"auc\",\n",
    "   \"eta\": 0.01, # 0.06, #0.01,\n",
    "   #\"min_child_weight\": 240,\n",
    "   \"subsample\": 0.75,\n",
    "   \"colsample_bytree\": 0.68,\n",
    "   \"max_depth\": 7\n",
    "}\n",
    "\n",
    "print('Load data...')\n",
    "train = imputed_dummified_train_x\n",
    "target = train_y\n",
    "test = pd.read_csv('data/test.csv')\n",
    "ids = test['ID'].values\n",
    "test = test.drop(['ID'],axis=1)\n",
    "#\n",
    "#Still disregarding v22\n",
    "test.drop('v22',axis = 1, inplace = True)\n",
    "\n",
    "print('Dummify...')\n",
    "test_dummy_columns = {}\n",
    "test_nulls_dict = {}\n",
    "\n",
    "for column in categorical_columns:\n",
    "    # print 'Dummifying ' + column\n",
    "    test,temp_dummy_list, temp_null_list = dummify(column,test)\n",
    "    test_dummy_columns[column] = temp_dummy_list\n",
    "    test_nulls_dict[column] = temp_null_list\n",
    "\n",
    "#Impute missing continuous variables\n",
    "print('Impute...')\n",
    "imp = Imputer(strategy='mean', axis=0)\n",
    "temp = imp.fit_transform(test)\n",
    "test_columns = test.columns\n",
    "test = pd.DataFrame(temp)\n",
    "test.columns = test_columns\n",
    "\n",
    "test_variables_to_predict = []\n",
    "for key in test_nulls_dict:\n",
    "    #print key + ' ' + str(len(test_nulls_dict[key]))\n",
    "    if len(test_nulls_dict[key]) > 0:\n",
    "        test_variables_to_predict.append(key)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict all categorical missing values\n",
    "for variable in test_variables_to_predict:\n",
    "    predict_values(test_dummy_columns[variable].tolist(),test, test_nulls_dict[variable], variable)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train.values, target.values, test_size=0.15)\n",
    "\n",
    "\n",
    "# xgtrain = xgb.DMatrix(train.values, target.values)\n",
    "# xgtest = xgb.DMatrix(test.values)\n",
    "\n",
    "xgtrain = xgb.DMatrix(X_train, y_train)\n",
    "xgvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "\n",
    "#Now let's fit the model\n",
    "print('Fit the model...')\n",
    "boost_round = 2000 #1800 CHANGE THIS BEFORE START\n",
    "\n",
    "filename = 'data/model/xgb.pkl'\n",
    "if not os.path.exists(filename):\n",
    "    print 'Training....'\n",
    "    eval_dict = dict()\n",
    "\n",
    "    clf = xgb.train(xgboost_params, xgtrain, num_boost_round=boost_round, verbose_eval=50, maximize=False,\n",
    "                    evals_result=eval_dict, early_stopping_rounds=100, evals=[(xgvalid, 'valid')])\n",
    "    # Use the following when not using validation a.k.a. for final run\n",
    "    #     clf = xgb.train(xgboost_params,xgtrain,num_boost_round=boost_round,verbose_eval=True,maximize=False)\n",
    "    joblib.dump(clf, filename)\n",
    "else:\n",
    "    print 'Loading...'\n",
    "    clf = joblib.load(filename)\n",
    "\n",
    "# ------------------\n",
    "#print(eval_dict)\n",
    "\n",
    "score_list = [float(s) for s in eval_dict['valid']['auc']]\n",
    "print(max(score_list))   # There are best score (max() - for 'auc', but min() for logloss)\n",
    "#same result:\n",
    "print(clf.best_score) \n",
    "#and best xgboost_round:\n",
    "print(clf.best_iteration)\n",
    "\n",
    "\n",
    "plt.plot(score_list)\n",
    "plt.show()\n",
    " \n",
    "#----------------    \n",
    "# Code for predicting on test data and saving prediction result\n",
    "# #Make predict\n",
    "# print('Predict...')\n",
    "# test_preds = clf.predict(xgtest, ntree_limit=clf.best_iteration)\n",
    "# # Save results\n",
    "# #\n",
    "# predictions_file = open(\"data/team_GAF_result.csv\", \"w\")\n",
    "# open_file_object = csv.writer(predictions_file)\n",
    "# open_file_object.writerow([\"ID\", \"PredictedProb\"])\n",
    "# open_file_object.writerows(zip(ids, test_preds))\n",
    "# predictions_file.close()\n",
    "# #\n",
    "# print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------    \n",
    "# Code for predicting on test data and saving prediction result\n",
    "# Make predict\n",
    "print('Predict...')\n",
    "filename = 'data/model/xgb.pkl'\n",
    "clf = joblib.load(filename)\n",
    "test_preds = clf.predict(xgtest, ntree_limit=clf.best_iteration)\n",
    "# Save results\n",
    "#\n",
    "predictions_file = open(\"data/team_GAF_result.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow([\"ID\", \"PredictedProb\"])\n",
    "open_file_object.writerows(zip(ids, test_preds))\n",
    "predictions_file.close()\n",
    "#\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
