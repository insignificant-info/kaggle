{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# XGBoost params:\n",
    "xgboost_params = { \n",
    "   \"objective\": \"binary:logistic\",\n",
    "   \"booster\": \"gbtree\",\n",
    "   \"eval_metric\": [\"auc\",\"error\",\"map\",\"logloss\"],\n",
    "   \"eta\": 0.01, # 0.06, #0.01,\n",
    "   #\"min_child_weight\": 240,\n",
    "   \"subsample\": 0.75,\n",
    "   \"colsample_bytree\": 0.68,\n",
    "   \"max_depth\": 7\n",
    "}\n",
    "\n",
    "print('Load data...')\n",
    "train = pd.read_csv('data/train.csv')\n",
    "target = train['target']\n",
    "train = train.drop(['ID','target'],axis=1)\n",
    "test = pd.read_csv('data/test.csv')\n",
    "ids = test['ID'].values\n",
    "test = test.drop(['ID'],axis=1)\n",
    "#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function, dummify, that will replace categorical features with dummy columns. Return the new dataset,\n",
    "# the names of the dummy columns, and the rows with null values for each categorical variable\n",
    "def dummify(name,series):\n",
    "    prefix_string = name + '_'\n",
    "    dummies = pd.get_dummies(series,prefix=prefix_string)\n",
    "    dummy_column_names = dummies.columns.values\n",
    "    #Get a list of all rows containing nulls. After dummifying these rows will just have all zeros for dummy variable\n",
    "    get_nulls = np.where(series.isnull() == True)[0].tolist()\n",
    "\n",
    "    return dummies, dummy_column_names, get_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop features with > 0.9 correlation. Keep feature with fewest NaNs\n",
    "\n",
    "corr = train.corr()\n",
    "\n",
    "to_drop = set()\n",
    "# for col in corr.columns.values:\n",
    "#     list_correlated = corr[col][(corr[col] > 0.9) & (corr[col] < 1)].index.tolist()\n",
    "#     if len(list_correlated) > 0:\n",
    "#         sys.stdout.write(col + \": \")\n",
    "#         print list_correlated\n",
    "for col in corr.columns.values:\n",
    "    if col in to_drop:\n",
    "        continue\n",
    "\n",
    "    col_list = corr[col][(corr[col] > 0.9) & (corr[col] < 1)].index.tolist()\n",
    "    col_set = set(col_list)\n",
    "    col_set.difference_update(to_drop)\n",
    "    if (len(col_list) == 0) or (len(col_set) == 0):\n",
    "        continue\n",
    "\n",
    "    col_list.append(col)\n",
    "    lowest_na_count = train[col_list[0]].isnull().sum()\n",
    "    best_col = col_list[0]\n",
    "    for option in col_list:\n",
    "        na_count = train[option].isnull().sum()\n",
    "        if na_count < lowest_na_count:\n",
    "            lowest_na_count = na_count\n",
    "            best_col = option\n",
    "    col_list.remove(best_col)\n",
    "#     print 'dropping = ' + str(col_list)\n",
    "#     print 'keeping = ' + str(best_col)\n",
    "    to_drop.update(col_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding features...\n"
     ]
    }
   ],
   "source": [
    "print \"Adding features...\"\n",
    "train['NA_num'] = train.isnull().sum(axis=1)\n",
    "test['NA_num'] = test.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop Correlated ...\n",
      "Clearing...\n",
      "Dummifying v3\n",
      "Dummifying v24\n",
      "Dummifying v30\n",
      "Dummifying v31\n",
      "Dummifying v47\n",
      "Dummifying v52\n",
      "Dummifying v56\n",
      "Dummifying v66\n",
      "Dummifying v71\n",
      "Dummifying v74\n",
      "Dummifying v75\n",
      "Dummifying v79\n",
      "Dummifying v91\n",
      "Dummifying v107\n",
      "Dummifying v110\n",
      "Dummifying v112\n",
      "Dummifying v113\n",
      "Dummifying v125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "drop_correlated = True\n",
    "\n",
    "if drop_correlated == True:\n",
    "    print 'Drop Correlated ...'\n",
    "    train.drop(list(to_drop),axis=1, inplace = True)\n",
    "    test.drop(list(to_drop),axis=1, inplace = True)\n",
    "\n",
    "print('Clearing...')\n",
    "train_dummy_columns = {}\n",
    "train_nulls_dict = {}\n",
    "test_dummy_columns = {}\n",
    "test_nulls_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "cleaned_train = train.copy()\n",
    "cleaned_test = test.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_name == 'v22':\n",
    "        #v22 has too many options to dummify, instead: factorize\n",
    "        cleaned_train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        cleaned_test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)    \n",
    "    elif train_series.dtype == 'O':\n",
    "        print 'Dummifying ' + train_name\n",
    "        cleaned_train.drop(train_name,axis=1,inplace = True)\n",
    "        cleaned_test.drop(train_name,axis=1,inplace = True)\n",
    "        \n",
    "        train_dummies, train_dummy_list, train_null_list = dummify(train_name,train_series)\n",
    "        test_dummies, test_dummy_list, test_null_list = dummify(test_name,test_series)\n",
    "\n",
    "        cleaned_train = pd.concat([cleaned_train,train_dummies], axis = 1)\n",
    "        cleaned_test = pd.concat([cleaned_test,test_dummies], axis = 1)\n",
    "\n",
    "        train_dummy_columns[train_name] = train_dummy_list\n",
    "        train_nulls_dict[train_name] = train_null_list\n",
    "        test_dummy_columns[test_name] = test_dummy_list\n",
    "        test_nulls_dict[test_name] = test_null_list\n",
    "        \n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            cleaned_train.loc[train_series.isnull(), train_name] = train_series.mean()\n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            cleaned_test.loc[test_series.isnull(), test_name] = train_series.mean()  #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cleaned_train.isnull().sum(axis=1)\n",
    "# with pd.option_context('display.max_rows', 999, 'display.max_columns', 3):\n",
    "#    print cleaned_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(cleaned_train.values, target.values, test_size=0.30, random_state = 1)\n",
    "\n",
    "X_valid, X_itest, y_valid, y_itest = train_test_split(X_valid, y_valid, test_size=0.20, random_state = 1)\n",
    "\n",
    "final = False\n",
    "\n",
    "# xgtrain = xgb.DMatrix(train.values, target.values)\n",
    "# xgtest = xgb.DMatrix(test.values)\n",
    "\n",
    "xgtrain = xgb.DMatrix(X_train, y_train)\n",
    "xgvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "xgtest = xgb.DMatrix(cleaned_test.values)\n",
    "\n",
    "if final == True:\n",
    "    xgtrain = xgb.DMatrix(cleaned_train.values, target.values)\n",
    "    \n",
    "\n",
    "#Now let's fit the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   learning_rate |   max_delta_step |   max_depth |   min_child_weight |   n_estimators |   subsample | \n"
     ]
    }
   ],
   "source": [
    "# michael.pearmain's bayesboost from https://github.com/mpearmain/BayesBoost/blob/master/examples/otto_boost.py\n",
    "\n",
    "def xgboostcv(max_depth,\n",
    "              learning_rate,\n",
    "              n_estimators,\n",
    "              gamma,\n",
    "              min_child_weight,\n",
    "              max_delta_step,\n",
    "              subsample,\n",
    "              colsample_bytree,\n",
    "              silent =True,\n",
    "              nthread = -1,\n",
    "              seed = 1234):\n",
    "    return cross_val_score(xgb.XGBClassifier(max_depth = int(max_depth),\n",
    "                                         learning_rate = learning_rate,\n",
    "                                         n_estimators = int(n_estimators),\n",
    "                                         silent = silent,\n",
    "                                         nthread = nthread,\n",
    "                                         gamma = gamma,\n",
    "                                         min_child_weight = min_child_weight,\n",
    "                                         max_delta_step = max_delta_step,\n",
    "                                         subsample = subsample,\n",
    "                                         colsample_bytree = colsample_bytree,\n",
    "                                         seed = seed,\n",
    "                                         objective = \"binary:logistic\"),\n",
    "                           X_train,\n",
    "                           y_train,\n",
    "                           \"log_loss\",\n",
    "                           cv=5).mean()\n",
    "\n",
    "\n",
    "xgboostBO = BayesianOptimization(xgboostcv,\n",
    "                                 {'max_depth': (5, 10),\n",
    "                                  'learning_rate': (0.01, 0.3),\n",
    "                                  'n_estimators': (50, 1000),\n",
    "                                  'gamma': (1., 0.01),\n",
    "                                  'min_child_weight': (2, 10),\n",
    "                                  'max_delta_step': (0, 0.1),\n",
    "                                  'subsample': (0.7, 0.8),\n",
    "                                  'colsample_bytree' :(0.5, 0.99)\n",
    "                                 })\n",
    "\n",
    "xgboostBO.maximize()\n",
    "print('-'*53)\n",
    "\n",
    "print('Final Results')\n",
    "print('XGBOOST: %f' % xgboostBO.res['max']['max_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train score:  0.377414274108\n",
    "# Valid score:  0.459356080264\n",
    "# Independent test set score:  0.460825726577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
